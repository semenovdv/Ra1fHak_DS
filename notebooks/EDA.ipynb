{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "mdir = Path(Path(os.path.abspath('')).parent)\n",
    "sys.path.append(str(mdir))\n",
    "from copy import deepcopy\n",
    "from copy import copy\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(mdir/'data'/'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(mdir/'data'/'test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "\n",
    "y_train = train.per_square_meter_price.values\n",
    "\n",
    "full_df = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "full_df.drop(['per_square_meter_price'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.date.min(), train.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.date.min(), test.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.price_type.unique(), test.price_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-competition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict final\n",
    "\n",
    "\n",
    "from raif_hack.model import BenchmarkModel\n",
    "from raif_hack.settings import MODEL_PARAMS, LOGGING_CONFIG, NUM_FEATURES, CATEGORICAL_OHE_FEATURES,CATEGORICAL_STE_FEATURES,TARGET\n",
    "\n",
    "from raif_hack.utils import PriceTypeEnum\n",
    "from raif_hack.metrics import metrics_stat\n",
    "from raif_hack.features import prepare_categorical, get_time_features, change_target_inflation, get_territory_features\n",
    "from raif_hack.features import get_random_feature, preproc_floors, fill_na, number_encode_features\n",
    "\n",
    "MY_FEATURES = ['basement',\n",
    " 'floor',\n",
    " 'mezzanine',\n",
    " 'num_floors',\n",
    " 'osm_city_nearest_name',\n",
    " 'per_square_meter_price',\n",
    " 'price_type',\n",
    " 'randNumCol',\n",
    " 'street',\n",
    " 'sum_build_001',\n",
    " 'sum_build_001_diff',\n",
    " 'sum_build_001_diff_regional',\n",
    " 'sum_build_001_region',\n",
    " 'sum_build_001_share',\n",
    " 'sum_build_001_share_regional',\n",
    " 'sum_build_005',\n",
    " 'sum_build_005_diff',\n",
    " 'sum_build_005_diff_regional',\n",
    " 'sum_build_005_region',\n",
    " 'sum_build_005_share',\n",
    " 'sum_build_005_share_regional',\n",
    " 'sum_build_0075',\n",
    " 'sum_build_0075_diff',\n",
    " 'sum_build_0075_diff_regional',\n",
    " 'sum_build_0075_region',\n",
    " 'sum_build_0075_share',\n",
    " 'sum_build_0075_share_regional',\n",
    " 'sum_build_01',\n",
    " 'sum_build_01_diff',\n",
    " 'sum_build_01_diff_regional',\n",
    " 'sum_build_01_region',\n",
    " 'sum_build_01_share',\n",
    " 'sum_build_01_share_regional',\n",
    " 'sum_other_001',\n",
    " 'sum_other_001_diff',\n",
    " 'sum_other_001_diff_regional',\n",
    " 'sum_other_001_region',\n",
    " 'sum_other_001_share',\n",
    " 'sum_other_001_share_regional',\n",
    " 'sum_other_005',\n",
    " 'sum_other_005_diff',\n",
    " 'sum_other_005_diff_regional',\n",
    " 'sum_other_005_region',\n",
    " 'sum_other_005_share',\n",
    " 'sum_other_005_share_regional',\n",
    " 'sum_other_0075',\n",
    " 'sum_other_0075_diff',\n",
    " 'sum_other_0075_diff_regional',\n",
    " 'sum_other_0075_region',\n",
    " 'sum_other_0075_share',\n",
    " 'sum_other_0075_share_regional',\n",
    " 'sum_other_01',\n",
    " 'sum_other_01_diff',\n",
    " 'sum_other_01_diff_regional',\n",
    " 'sum_other_01_region',\n",
    " 'sum_other_01_share',\n",
    " 'sum_other_01_share_regional',\n",
    " 'tech' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_df = pd.read_csv(mdir/'data'/'train.csv')\n",
    "test_df = pd.read_csv(mdir/'data'/'test.csv')\n",
    "\n",
    "train_df['is_train'] = 1\n",
    "test_df['is_train'] = 0\n",
    "\n",
    "full_df = pd.concat([train_df, test_df])\"\"\"\n",
    "\n",
    "\n",
    "full_df = get_time_features(full_df)\n",
    "\n",
    "full_df = get_territory_features(full_df)\n",
    "\n",
    "# попробовать на честность\n",
    "#full_df = change_target_inflation(full_df)\n",
    "\n",
    "full_df = preproc_floors(full_df)\n",
    "full_df = fill_na(full_df)\n",
    "\n",
    "full_df = get_random_feature(full_df)\n",
    "\n",
    "#full_df, _ = number_encode_features(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df[\"per_square_meter_price\"] = np.log1p(full_df[\"per_square_meter_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df = \n",
    "#pd.get_dummies()\n",
    "full_df[full_df.columns[full_df.dtypes == 'object']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "\n",
    "def number_encode_features(df):\n",
    "    result = df.copy() \n",
    "    result['street'] += result['city']\n",
    "    encoders = {}\n",
    "    for column in result.columns[result.dtypes == 'object'].drop(['id']):\n",
    "        encoders[column] = preprocessing.LabelEncoder() \n",
    "        result[column] = encoders[column].fit_transform(result[column])\n",
    "    return result, encoders\n",
    "full_df, _ = number_encode_features(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = full_df.columns.drop(['id','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_na = (full_df.isnull().sum() / len(full_df)) * 100\n",
    "full_df_na = full_df_na.drop(full_df_na[full_df_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :full_df_na})\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.fillna(value = full_df['osm_city_nearest_population'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_na = (full_df.isnull().sum() / len(full_df)) * 100\n",
    "full_df_na = full_df_na.drop(full_df_na[full_df_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :full_df_na})\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = full_df[:ntrain]\n",
    "#test = full_df[ntrain:]\n",
    "train_df = full_df[full_df['is_train'] == 1][model_cols]\n",
    "test_df = full_df[full_df['is_train'] == 0][model_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_df.values)\n",
    "    # y_train_log, y_train\n",
    "    #rmse= np.sqrt(-cross_val_score(model, train_df.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    rmse= np.sqrt(-cross_val_score(model, train_df.values, y_train_log, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1, normalize=True, max_iter=1000000))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3, normalize=True, max_iter=1000000))\n",
    "#KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5,)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "model_cat = CatBoostRegressor(iterations=30000,\n",
    "                          loss_function = 'RMSE',\n",
    "                          task_type=\"CPU\",\n",
    "                          depth=4  # - 81480.24422\n",
    "                         )\n",
    "grid = {'max_depth': [3,4,5],'n_estimators':[100, 200, 300]}\n",
    "\n",
    "#Instantiate GridSearchCV\n",
    "gscv = GridSearchCV (estimator = cbc, param_grid = grid, scoring ='accuracy', cv = 5)\n",
    "\n",
    "#fit the model\n",
    "gscv.fit(X,y)\n",
    "\n",
    "#returns the estimator with the best performance\n",
    "print(gscv.best_estimator_)\n",
    "\n",
    "#returns the best score\n",
    "print(gscv.best_score_)\n",
    "\n",
    "#returns the best parameters\n",
    "print(gscv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lasso score: 1.2285 (0.0390)\n",
    "# Lasso score: 1.2287 (0.0407)   normalize=True,max_iter=1000000))  - \n",
    "\n",
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-heather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(model_cat)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df, y_train_log, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = CatBoostRegressor(iterations=30000,\n",
    "                          loss_function = 'MAPE',\n",
    "                          task_type=\"CPU\",\n",
    "                          depth=4  # - 81480.24422\n",
    "                         )\n",
    "\n",
    "\n",
    "model_cat.fit(X = np.array(X_train),\n",
    "            y = np.array(y_train),\n",
    "          eval_set = (np.array(X_test), np.array(y_test)),\n",
    "            #silent = False,\n",
    "          verbose = 500,\n",
    "            early_stopping_rounds=500, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = ['wc','ed','ms','occ','rel','race','sex','nc','label']\n",
    "for header in category_cols:\n",
    "    train_set[header] = train_set[header].astype('category').cat.codes\n",
    "    test_set[header] = test_set[header].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "EARLY_STOPPING_ROUND = 500\n",
    "\n",
    "def objective(trial):\n",
    "    param = {}\n",
    "    param['learning_rate'] = trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.02, 0.001)\n",
    "    param['depth'] = trial.suggest_int('depth', 9, 15)\n",
    "    param['l2_leaf_reg'] = trial.suggest_discrete_uniform('l2_leaf_reg', 1.0, 5.5, 0.5)\n",
    "    param['min_child_samples'] = trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32])\n",
    "    param['grow_policy'] = 'Depthwise'\n",
    "    param['iterations'] = 30000\n",
    "    param['use_best_model'] = True\n",
    "    param['eval_metric'] = 'RMSE'\n",
    "    param['od_type'] = 'iter'\n",
    "    param['od_wait'] = 20\n",
    "    param['random_state'] = RANDOM_SEED\n",
    "    #param['logging_level'] = 'Silent'\n",
    "    param['verbose'] = 500\n",
    "    \n",
    "    regressor = CatBoostRegressor(**param)\n",
    "\n",
    "    regressor.fit(X_train.copy(), y_train.copy(),\n",
    "                  eval_set=[(X_test.copy(), y_test.copy())],\n",
    "                  early_stopping_rounds=EARLY_STOPPING_ROUND)\n",
    "    loss = mean_squared_error(y_test, regressor.predict(X_test.copy()))\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(study_name=f'catboost-seed{RANDOM_SEED}')\n",
    "study.optimize(objective, n_trials=30000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_cat.predict(test_df)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['per_square_meter_price'] = np.e**res*0.95\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models.fit(train.values, y_train)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n",
    "print(rmsle(y_train, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RMSE on the entire Train data when averaging'''\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(y_train,stacked_train_pred*0.70 +\n",
    "               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['SalePrice'] = np.e**res\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = full_df[full_df['is_train'] == 1]\n",
    "test_df = full_df[full_df['is_train'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ID = train['id']\n",
    "test_ID = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "#import pandas_profiling as pp # подсмотрел в kernels, интересно\n",
    "\n",
    "import tensorflow as tf\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error\n",
    "\n",
    "THRESHOLD = 0.15\n",
    "NEGATIVE_WEIGHT = 1.1\n",
    "def deviation_metric_one_sample(y_true: typing.Union[float, int], y_pred: typing.Union[float, int]) -> float:\n",
    "    \"\"\"\n",
    "    Реализация кастомной метрики для хакатона.\n",
    "\n",
    "    :param y_true: float, реальная цена\n",
    "    :param y_pred: float, предсказанная цена\n",
    "    :return: float, значение метрики\n",
    "    \"\"\"\n",
    "    deviation = (y_pred - y_true) / np.maximum(1e-8, y_true)\n",
    "    if np.abs(deviation) <= THRESHOLD:\n",
    "        return 0\n",
    "    elif deviation <= - 4 * THRESHOLD:\n",
    "        return 9 * NEGATIVE_WEIGHT\n",
    "    elif deviation < -THRESHOLD:\n",
    "        return NEGATIVE_WEIGHT * ((deviation / THRESHOLD) + 1) ** 2\n",
    "    elif deviation < 4 * THRESHOLD:\n",
    "        return ((deviation / THRESHOLD) - 1) ** 2\n",
    "    else:\n",
    "        return 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-advance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_features = ['lat',\n",
    " 'lng',\n",
    " 'osm_amenity_points_in_0.001',\n",
    " 'osm_amenity_points_in_0.005',\n",
    " 'osm_amenity_points_in_0.0075',\n",
    " 'osm_building_points_in_0.005',\n",
    " 'osm_building_points_in_0.0075',\n",
    " 'osm_building_points_in_0.01',\n",
    " 'osm_catering_points_in_0.001',\n",
    " 'osm_catering_points_in_0.005',\n",
    " 'osm_catering_points_in_0.0075',\n",
    " 'osm_catering_points_in_0.01',\n",
    " 'osm_city_closest_dist',\n",
    " 'osm_city_nearest_population',\n",
    " 'osm_crossing_closest_dist',\n",
    " 'osm_crossing_points_in_0.001',\n",
    " 'osm_crossing_points_in_0.005',\n",
    " 'osm_crossing_points_in_0.0075',\n",
    " 'osm_crossing_points_in_0.01',\n",
    " 'osm_culture_points_in_0.005',\n",
    " 'osm_culture_points_in_0.0075',\n",
    " 'osm_culture_points_in_0.01',\n",
    " 'osm_finance_points_in_0.001',\n",
    " 'osm_finance_points_in_0.005',\n",
    " 'osm_finance_points_in_0.0075',\n",
    " 'osm_finance_points_in_0.01',\n",
    " 'osm_healthcare_points_in_0.005',\n",
    " 'osm_healthcare_points_in_0.0075',\n",
    " 'osm_healthcare_points_in_0.01',\n",
    " 'osm_historic_points_in_0.005',\n",
    " 'osm_historic_points_in_0.0075',\n",
    " 'osm_historic_points_in_0.01',\n",
    " 'osm_hotels_points_in_0.0075',\n",
    " 'osm_hotels_points_in_0.01',\n",
    " 'osm_leisure_points_in_0.005',\n",
    " 'osm_leisure_points_in_0.0075',\n",
    " 'osm_leisure_points_in_0.01',\n",
    " 'osm_offices_points_in_0.001',\n",
    " 'osm_offices_points_in_0.005',\n",
    " 'osm_offices_points_in_0.0075',\n",
    " 'osm_offices_points_in_0.01',\n",
    " 'osm_shops_points_in_0.001',\n",
    " 'osm_shops_points_in_0.005',\n",
    " 'osm_shops_points_in_0.0075',\n",
    " 'osm_shops_points_in_0.01',\n",
    " 'osm_subway_closest_dist',\n",
    " 'osm_train_stop_closest_dist',\n",
    " 'osm_train_stop_points_in_0.005',\n",
    " 'osm_transport_stop_closest_dist',\n",
    " 'osm_transport_stop_points_in_0.005',\n",
    " 'osm_transport_stop_points_in_0.0075',\n",
    " 'osm_transport_stop_points_in_0.01',\n",
    " 'reform_count_of_houses_1000',\n",
    " 'reform_count_of_houses_500',\n",
    " 'reform_house_population_1000',\n",
    " 'reform_house_population_500',\n",
    " 'reform_mean_floor_count_1000',\n",
    " 'reform_mean_floor_count_500',\n",
    " 'reform_mean_year_building_1000',\n",
    " 'reform_mean_year_building_500',\n",
    " 'region',\n",
    " 'total_square',\n",
    " 'street',\n",
    " 'realty_type',\n",
    " 'day',\n",
    " 'dayofyear',\n",
    " 'sum_other_001',\n",
    " 'sum_other_005',\n",
    " 'sum_other_001_diff',\n",
    " 'sum_other_005_diff',\n",
    " 'sum_other_0075_diff',\n",
    " 'sum_other_001_share',\n",
    " 'sum_other_005_share',\n",
    " 'sum_other_001_region',\n",
    " 'sum_other_005_region',\n",
    " 'sum_other_0075_region',\n",
    " 'sum_other_01_region',\n",
    " 'sum_other_001_diff_regional',\n",
    " 'sum_other_005_diff_regional',\n",
    " 'sum_other_0075_diff_regional',\n",
    " 'sum_other_01_diff_regional',\n",
    " 'sum_other_001_share_regional',\n",
    " 'sum_other_005_share_regional',\n",
    " 'sum_other_0075_share_regional',\n",
    " 'sum_other_01_share_regional',\n",
    " 'sum_build_005',\n",
    " 'sum_build_005_share',\n",
    " 'sum_build_005_region',\n",
    " 'sum_build_0075_region',\n",
    " 'sum_build_01_region',\n",
    " 'sum_build_005_diff_regional',\n",
    " 'sum_build_01_diff_regional',\n",
    " 'sum_build_005_share_regional',\n",
    " 'sum_build_01_share_regional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "X = train_df.drop(['per_square_meter_price','date', 'id', 'is_train'], axis=1)[true_features]\n",
    "Y = train_df[['per_square_meter_price']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-spare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(iterations=30000,\n",
    "                          loss_function = 'RMSE',\n",
    "                          task_type=\"CPU\",\n",
    "                          depth=6  # - 81480.24422\n",
    "                         )\n",
    "                          #devices='0:1',\n",
    "                          #learning_rate= 0.3,\n",
    "                          #depth = 14,\n",
    "                          #l2_leaf_reg = 1\n",
    "                        \n",
    "\n",
    "model.fit(X = np.array(X_train),\n",
    "            y = np.array(y_train),\n",
    "          eval_set = (np.array(X_test), np.array(y_test)),\n",
    "            #silent = False,\n",
    "          verbose = 500,\n",
    "            early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-madonna",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtest = test_df.drop(['per_square_meter_price','date'], axis=1)\n",
    "mtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_cat.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "THRESHOLD = 0.15\n",
    "NEGATIVE_WEIGHT = 1.1\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def deviation_metric_one_sample(y_true: typing.Union[float, int], y_pred: typing.Union[float, int]) -> float:\n",
    "    \"\"\"\n",
    "    Реализация кастомной метрики для хакатона.\n",
    "\n",
    "    :param y_true: float, реальная цена\n",
    "    :param y_pred: float, предсказанная цена\n",
    "    :return: float, значение метрики\n",
    "    \"\"\"\n",
    "    deviation = (y_pred - y_true) / np.maximum(1e-8, y_true)\n",
    "    if np.abs(deviation) <= THRESHOLD:\n",
    "        return 0\n",
    "    elif deviation <= - 4 * THRESHOLD:\n",
    "        return 9 * NEGATIVE_WEIGHT\n",
    "    elif deviation < -THRESHOLD:\n",
    "        return NEGATIVE_WEIGHT * ((deviation / THRESHOLD) + 1) ** 2\n",
    "    elif deviation < 4 * THRESHOLD:\n",
    "        return ((deviation / THRESHOLD) - 1) ** 2\n",
    "    else:\n",
    "        return 9\n",
    "\n",
    "\n",
    "def deviation_metric(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.array([deviation_metric_one_sample(y_true[n], y_pred[n]) for n in range(len(y_true))]).mean()\n",
    "\n",
    "def median_absolute_percentage_error(y_true: np.array, y_pred: np.array) -> float:\n",
    "    return np.median(np.abs(y_pred-y_true)/y_true)\n",
    "\n",
    "def metrics_stat(y_true: np.array, y_pred: np.array) -> typing.Dict[str,float]:\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mdape = median_absolute_percentage_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    raif_metric = deviation_metric(y_true, y_pred)\n",
    "    return {'mape':mape, 'mdape':mdape, 'rmse': rmse, 'r2': r2, 'raif_metric':raif_metric}\n",
    "\n",
    "EPS = 1e-8\n",
    "assert deviation_metric(np.array([1,2,3,4,5]),np.array([1,2,3,4,5])) <= EPS\n",
    "assert deviation_metric(np.array([1,2,3,4,5]),np.array([0.9,1.8,2.7,3.6,4.5])) <= EPS\n",
    "assert deviation_metric(np.array([1,2,3,4,5]),np.array([1.1,2.2,3.3,4.4,5.5])) <= EPS\n",
    "assert deviation_metric(np.array([1,2,3,4,5]),np.array([1.15,2.3,3.45,4.6,5.75])) <= EPS\n",
    "assert np.abs(deviation_metric(np.array([1,2,3,4,5]),np.array([1.3,2.6,3.9,5.2,6.5]))-1) <= EPS\n",
    "assert np.abs(deviation_metric(np.array([1,2,3,4,5]),np.array([0.7,1.4,2.1,2.8,3.5]))-1*NEGATIVE_WEIGHT) <= EPS\n",
    "assert np.abs(deviation_metric(np.array([1,2,3,4,5]),np.array([10,20,30,40,50]))-9) <= EPS\n",
    "assert np.abs(deviation_metric(np.array([1,2,3,4,5]),np.array([0,0,0,0,0]))-9*NEGATIVE_WEIGHT) <= EPS\n",
    "assert np.abs(deviation_metric(np.array([1,2,3,4,5]),np.array([1,2.2,3.3,5,50])) - 85/45) <= EPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"{'mape': 1.5331790967384054,\n",
    " 'mdape': 0.28967585280398345,\n",
    " 'rmse': 74071.48571000711,\n",
    " 'r2': 0.8060378887712242,\n",
    " 'raif_metric': 3.3111947615740123}\"\"\"\n",
    "metrics_stat(y_test, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(np.array(test_df.drop(['per_square_meter_price','date', 'id', 'is_train'], axis=1)[true_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['price_type'] == 1].per_square_meter_price.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.e**res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-event",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_push = test_df[['id']]\n",
    "to_push['per_square_meter_price'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = pd.read_csv(mdir/'data'/'test_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = test_sub[['id']].merge(to_push, how='left', on='id')\n",
    "test_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub.to_csv('res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_, model.feature_names_, test_df.drop(['per_square_meter_price','date', 'id', 'is_train'], axis=1).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = []\n",
    "feat = []\n",
    "for x,y in zip(test_df.drop(['per_square_meter_price','date', 'id', 'is_train'], axis=1).columns, model.feature_importances_):\n",
    "      if y > 0.2101:\n",
    "            imp.append((x,y))\n",
    "            feat.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.sort(key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_test = pd.read_csv(mdir/'data'/'test.csv')\n",
    "to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['id'] = to_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_push = test_df[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_push['per_square_meter_price'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_push.to_csv('res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-attribute",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
